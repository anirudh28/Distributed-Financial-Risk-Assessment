{
  "metadata": {
    "name": "project-data-cleaning",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val filePath \u003d \"/user/nk3853_nyu_edu/stocks/20MICRONS.NS.csv\""
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val rawDf \u003d spark.read.option(\"header\",false).option(\"inferSchema\",\"true\").csv(filePath)\nval filteredRDD \u003d rawDf.rdd.zipWithIndex().filter { case (_, idx) \u003d\u003e idx \u003e\u003d 3 }.map(x \u003d\u003e x._1)\nval filteredDf \u003d spark.createDataFrame(filteredRDD, rawDf.schema)\nval columnNames \u003d Seq(\"Date\", \"AdjClose\", \"Close\", \"Open\", \"High\", \"Low\", \"Volume\")\nval finalDf \u003d filteredDf.toDF(columnNames: _*)\n\nfinalDf.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val selectedDf \u003d finalDf.select($\"Date\", $\"AdjClose\")\nval formattedDf \u003d selectedDf.withColumn(\"AdjClose\", $\"AdjClose\".cast(\"double\")).withColumn(\"Date\", to_date($\"Date\", \"yyyy-MM-dd\"))\nval adjustedDf \u003d formattedDf.withColumn(\"AdjClose\", when($\"AdjClose\" \u003d\u003d\u003d 0, lit(null)).otherwise($\"AdjClose\"))\n"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.expressions.Window\nval windowSpec \u003d Window.orderBy(\"Date\")\nval filledDf \u003d adjustedDf.withColumn( \"AdjClose\", coalesce( last($\"AdjClose\", ignoreNulls \u003d true).over(windowSpec), first($\"AdjClose\", ignoreNulls \u003d true).over(windowSpec)))\nfilledDf.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "filledDf.describe().show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val rowCount \u003d filledDf.count()\nval nullCounts \u003d adjustedDf.select(\n  adjustedDf.columns.map(c \u003d\u003e\n    count(when(col(c).isNull, 1)).alias(s\"${c}_nulls\")\n  ): _*\n)\nnullCounts.show()\n"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import spark.implicits._\nimport org.apache.spark.sql.functions._\n// Outlier detection based on 3 standard deviations\nval stats \u003d filledDf.agg(\n  avg($\"AdjClose\").alias(\"mean\"),\n  stddev(col(\"AdjClose\")).alias(\"std_dev\")\n).first()\n\nval mean \u003d stats.getAs[Double](\"mean\")\nval std_dev \u003d stats.getAs[Double](\"std_dev\")\n\nval cleanedDf \u003d filledDf.filter($\"AdjClose\" \u003e mean + 3 * std_dev || $\"AdjClose\" \u003c mean - 3 * std_dev)\ncleanedDf.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputPath \u003d \"/user/nk3853_nyu_edu/stocks/cleaned/20MICRONS.NS\"\ncleanedDf.coalesce(1).write\n  .mode(\"overwrite\")\n  .option(\"header\", \"true\")\n  .csv(outputPath)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.hadoop.fs.{FileSystem, Path}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.{Row, SparkSession}\n\nval directoryPath \u003d \"/user/nk3853_nyu_edu/stocks\"\nval fs \u003d FileSystem.get(spark.sparkContext.hadoopConfiguration)\nval stockFiles \u003d fs.listStatus(new Path(directoryPath)).filter(_.getPath.getName.endsWith(\".csv\")).map(_.getPath.toString)\n\nval results \u003d stockFiles.map { filePath \u003d\u003e\n  val stockName \u003d filePath.split(\"/\").last.stripSuffix(\".L.csv\")\n  \n  val rawDf \u003d spark.read.option(\"header\", false).option(\"inferSchema\", \"true\").csv(filePath)\n  val filteredRDD \u003d rawDf.rdd.zipWithIndex().filter { case (_, idx) \u003d\u003e idx \u003e\u003d 3 }.map(_._1)\n  val filteredDf \u003d spark.createDataFrame(filteredRDD, rawDf.schema)\n  val columnNames \u003d Seq(\"Date\", \"AdjClose\", \"Close\", \"Open\", \"High\", \"Low\", \"Volume\")\n  val finalDf \u003d filteredDf.toDF(columnNames: _*)\n  val selectedDf \u003d finalDf.select($\"Date\", $\"Close\").withColumn(\"Date\", to_date($\"Date\", \"yyyy-MM-dd\")).withColumn(\"Close\", $\"Close\".cast(\"double\"))\n\n  val startDate \u003d selectedDf.agg(min(\"Date\")).collect().head.getDate(0)\n  val minValue \u003d selectedDf.agg(min(\"Close\")).collect().head.getDouble(0)\n  val maxValue \u003d selectedDf.agg(max(\"Close\")).collect().head.getDouble(0)\n  val nullCount \u003d selectedDf.filter($\"Close\".isNull).count()\n  val stdDevRow \u003d selectedDf.agg(stddev(\"Close\")).collect().head\n  val stdDev \u003d if (stdDevRow.isNullAt(0)) Double.NaN else stdDevRow.getDouble(0)\n\n\n  (stockName, startDate, minValue, maxValue, nullCount, stdDev)\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Profiling\n\nimport spark.implicits._\nval resultsDf \u003d results.toSeq.toDF(\"StockSymbol\", \"StartDate\", \"MinValue\", \"MaxValue\", \"NullCount\", \"StdDev\")\n\nval valueDistribution \u003d resultsDf.select(\"*\")\nvalueDistribution.createOrReplaceTempView(\"value_distribution\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT StockSymbol, StartDate, MaxValue, MinValue, NullCount, StdDev FROM value_distribution"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    YEAR(CAST(StartDate AS DATE)) AS Year, \n    COUNT(DISTINCT StockSymbol) AS StockCount\nFROM value_distribution\nWHERE YEAR(CAST(StartDate AS DATE)) BETWEEN 2000 AND 2024\nGROUP BY Year\nORDER BY Year\n"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\n\n\nstockFiles.foreach { filePath \u003d\u003e\n  val stockName \u003d filePath.split(\"/\").last.stripSuffix(\".L.csv\")\n\n  val rawDf \u003d spark.read.option(\"header\", false).option(\"inferSchema\", \"true\").csv(filePath)\n  val filteredRDD \u003d rawDf.rdd.zipWithIndex().filter { case (_, idx) \u003d\u003e idx \u003e\u003d 3 }.map(_._1)\n  val filteredDf \u003d spark.createDataFrame(filteredRDD, rawDf.schema)\n  val columnNames \u003d Seq(\"Date\", \"AdjClose\", \"Close\", \"Open\", \"High\", \"Low\", \"Volume\")\n  val finalDf \u003d filteredDf.toDF(columnNames: _*)\n\n  val selectedDf \u003d finalDf.select($\"Date\", $\"Close\").withColumn(\"Date\", to_date($\"Date\", \"yyyy-MM-dd\")).withColumn(\"Close\", $\"Close\".cast(\"double\"))\n\n\n  val forwardFillSpec \u003d Window.orderBy(\"Date\").rowsBetween(Window.unboundedPreceding, 0)\n  val backwardFillSpec \u003d Window.orderBy(\"Date\").rowsBetween(0, Window.unboundedFollowing)\n\n  val cleanedDf \u003d selectedDf.withColumn(\"Close\", last($\"Close\", ignoreNulls \u003d true).over(forwardFillSpec)).withColumn(\"Close\", coalesce($\"Close\", first($\"Close\", ignoreNulls \u003d true).over(backwardFillSpec)))\n\n  cleanedDf.write.option(\"header\", \"true\").mode(\"overwrite\").csv(s\"/user/nk3853_nyu_edu/stocks_cleaned/$stockName\")\n}\n"
    }
  ]
}