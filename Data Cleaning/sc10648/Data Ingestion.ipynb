{
  "metadata": {
    "name": "Data Ingestion",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\nimport org.apache.hadoop.fs.{FileSystem, Path}\nimport java.nio.file.{Files, Paths, StandardCopyOption}\nimport java.io.PrintWriter"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val directoryPath \u003d \"/user/sc10648_nyu_edu/stocks/\"\n\nval fs \u003d FileSystem.get(spark.sparkContext.hadoopConfiguration)\n\nval csvFiles \u003d fs.listStatus(new Path(directoryPath)).filter(_.getPath.getName.endsWith(\".csv\")).map(_.getPath.toString).take(20)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "var allMetricsData \u003d Seq.empty[(String, String, Double, Double, Double, Double, Long, Long)]"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "def processAndWriteToNewFolder(filePath: String, outputDir: String): Unit \u003d {\n    \n    val rawDf \u003d spark.read.option(\"header\", false).option(\"inferSchema\", \"true\").csv(filePath)\n  \n    val filteredRDD \u003d rawDf.rdd.zipWithIndex().filter { case (_, idx) \u003d\u003e idx \u003e\u003d 3 }.map(_._1)\n    val filteredDf \u003d spark.createDataFrame(filteredRDD, rawDf.schema)\n\n    val columnNames \u003d Seq(\"Date\", \"AdjClose\", \"Close\", \"Open\", \"High\", \"Low\", \"Volume\")\n    val finalDf \u003d filteredDf.toDF(columnNames: _*)\n\n    val selectedDf \u003d finalDf.select($\"Date\", $\"Close\")\n    val formattedDf \u003d selectedDf.withColumn(\"Close\", $\"Close\".cast(\"double\")).withColumn(\"Date\", to_date($\"Date\", \"yyyy-MM-dd\"))\n  \n    val startDate \u003d formattedDf.select(min($\"Date\")).collect()(0)(0)\n    val minClose \u003d formattedDf.select(min($\"Close\")).collect()(0)(0).asInstanceOf[Double]\n    val maxClose \u003d formattedDf.select(max($\"Close\")).collect()(0)(0).asInstanceOf[Double]\n    val avgClose \u003d formattedDf.select(avg($\"Close\")).collect()(0)(0).asInstanceOf[Double]\n    val stdDevClose \u003d formattedDf.select(stddev($\"Close\")).collect()(0)(0).asInstanceOf[Double]\n    val nullCount \u003d formattedDf.filter($\"Close\".isNull).count()\n    val zeroCount \u003d formattedDf.filter($\"Close\" \u003d\u003d\u003d 0).count()\n  \n    \n    val fileNameWithoutExtension \u003d filePath.split(\"/\").last.replace(\".csv\", \"\")\n    \n    allMetricsData :+\u003d (fileNameWithoutExtension, startDate.toString, minClose, maxClose, avgClose, stdDevClose, nullCount, zeroCount)\n    \n    val deduplicatedDf \u003d formattedDf.dropDuplicates(\"Date\")\n\n    val adjustedDf \u003d deduplicatedDf.withColumn(\"Close\", when($\"Close\" \u003d\u003d\u003d 0, lit(null)).otherwise($\"Close\"))\n\n    val windowSpec \u003d Window.orderBy(\"Date\")\n    val filledDf \u003d adjustedDf.withColumn(\"Close\",coalesce(last($\"Close\", ignoreNulls \u003d true).over(windowSpec), first($\"Close\", ignoreNulls \u003d true).over(windowSpec)))\n  \n    val fileName2 \u003d filePath.split(\"/\").last.replace(\".csv\", \"\")\n    val outputPath \u003d s\"$outputDir/$fileName2\"\n  \n    filledDf.write.mode(\"overwrite\").option(\"header\", \"false\").csv(outputPath)\n\n}\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val outputDir \u003d \"/user/sc10648_nyu_edu/stocks_processed\"\n\nval outputPath \u003d new Path(outputDir)\nif (!fs.exists(outputPath)) {\n  fs.mkdirs(outputPath)\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "csvFiles.foreach(file \u003d\u003e processAndWriteToNewFolder(file, outputDir))"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "val metricsDf \u003d spark.createDataFrame(allMetricsData).toDF(\"File\", \"StartDate\", \"MinClose\", \"MaxClose\", \"AvgClose\", \"StdDevClose\",\"NullCount\", \"ZeroCount\")\n\nmetricsDf.show(false)"
    }
  ]
}