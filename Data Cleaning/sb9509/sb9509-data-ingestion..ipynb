{
  "metadata": {
    "name": "sb9509-data-ingestion",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.hadoop.fs.{FileSystem, Path}\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.{Row, SparkSession}\n\nval directoryPath \u003d \"/user/sb9509_nyu_edu/stocks\"\nval fs \u003d FileSystem.get(spark.sparkContext.hadoopConfiguration)\nval stockFiles \u003d fs.listStatus(new Path(directoryPath)).filter(_.getPath.getName.endsWith(\".csv\")).map(_.getPath.toString)\n\nval results \u003d stockFiles.map { filePath \u003d\u003e\n  val stockName \u003d filePath.split(\"/\").last.stripSuffix(\".L.csv\")\n  \n  val rawDf \u003d spark.read.option(\"header\", false).option(\"inferSchema\", \"true\").csv(filePath)\n  val filteredRDD \u003d rawDf.rdd.zipWithIndex().filter { case (_, idx) \u003d\u003e idx \u003e\u003d 3 }.map(_._1)\n  val filteredDf \u003d spark.createDataFrame(filteredRDD, rawDf.schema)\n  val columnNames \u003d Seq(\"Date\", \"AdjClose\", \"Close\", \"Open\", \"High\", \"Low\", \"Volume\")\n  val finalDf \u003d filteredDf.toDF(columnNames: _*)\n  val selectedDf \u003d finalDf.select($\"Date\", $\"Close\").withColumn(\"Date\", to_date($\"Date\", \"yyyy-MM-dd\")).withColumn(\"Close\", $\"Close\".cast(\"double\"))\n\n  val startDate \u003d selectedDf.agg(min(\"Date\")).collect().head.getDate(0)\n  val minValue \u003d selectedDf.agg(min(\"Close\")).collect().head.getDouble(0)\n  val maxValue \u003d selectedDf.agg(max(\"Close\")).collect().head.getDouble(0)\n  val nullCount \u003d selectedDf.filter($\"Close\".isNull).count()\n  val stdDevRow \u003d selectedDf.agg(stddev(\"Close\")).collect().head\n  val stdDev \u003d if (stdDevRow.isNullAt(0)) Double.NaN else stdDevRow.getDouble(0)\n\n\n  (stockName, startDate, minValue, maxValue, nullCount, stdDev)\n}\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "//Profiling\n\nimport spark.implicits._\nval resultsDf \u003d results.toSeq.toDF(\"StockSymbol\", \"StartDate\", \"MinValue\", \"MaxValue\", \"NullCount\", \"StdDev\")\n\nval valueDistribution \u003d resultsDf.select(\"*\")\nvalueDistribution.createOrReplaceTempView(\"value_distribution\")"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT StockSymbol, StartDate, MaxValue, MinValue, NullCount, StdDev FROM value_distribution"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%sql\nSELECT \n    YEAR(CAST(StartDate AS DATE)) AS Year, \n    COUNT(DISTINCT StockSymbol) AS StockCount\nFROM value_distribution\nWHERE YEAR(CAST(StartDate AS DATE)) BETWEEN 2000 AND 2024\nGROUP BY Year\nORDER BY Year\n"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "import org.apache.spark.sql.functions._\nimport org.apache.spark.sql.expressions.Window\n\n\nstockFiles.foreach { filePath \u003d\u003e\n  val stockName \u003d filePath.split(\"/\").last.stripSuffix(\".L.csv\")\n\n  val rawDf \u003d spark.read.option(\"header\", false).option(\"inferSchema\", \"true\").csv(filePath)\n  val filteredRDD \u003d rawDf.rdd.zipWithIndex().filter { case (_, idx) \u003d\u003e idx \u003e\u003d 3 }.map(_._1)\n  val filteredDf \u003d spark.createDataFrame(filteredRDD, rawDf.schema)\n  val columnNames \u003d Seq(\"Date\", \"AdjClose\", \"Close\", \"Open\", \"High\", \"Low\", \"Volume\")\n  val finalDf \u003d filteredDf.toDF(columnNames: _*)\n\n  val selectedDf \u003d finalDf.select($\"Date\", $\"Close\").withColumn(\"Date\", to_date($\"Date\", \"yyyy-MM-dd\")).withColumn(\"Close\", $\"Close\".cast(\"double\"))\n\n\n  val forwardFillSpec \u003d Window.orderBy(\"Date\").rowsBetween(Window.unboundedPreceding, 0)\n  val backwardFillSpec \u003d Window.orderBy(\"Date\").rowsBetween(0, Window.unboundedFollowing)\n\n  val cleanedDf \u003d selectedDf.withColumn(\"Close\", last($\"Close\", ignoreNulls \u003d true).over(forwardFillSpec)).withColumn(\"Close\", coalesce($\"Close\", first($\"Close\", ignoreNulls \u003d true).over(backwardFillSpec)))\n\n  cleanedDf.write.option(\"header\", \"true\").mode(\"overwrite\").csv(s\"/user/sb9509_nyu_edu/stocks_cleaned/$stockName\")\n}\n"
    }
  ]
}